---
params:
  object: NA
  name: NULL
title: "Benchmarking report for `r params$name` "
author: created by challengeR `r packageVersion('challengeR')` (Wiesenfarth, Reinke, Cardoso, Maier-Hein & Kopp-Schneider, 2019)
date: "`r Sys.setlocale('LC_TIME', 'English'); format(Sys.time(), '%d %B, %Y')`"
editor_options: 
  chunk_output_type: console
---




```{r setup, include=FALSE}
options(width=80)
# out.format <- knitr::opts_knit$get("out.format")
# img_template <- switch( out.format,
#                      word = list("img-params"=list(fig.width=6,
#                                                    fig.height=6,
#                                                    dpi=150)),
#                      {
#                        # default
#                        list("img-params"=list( dpi=150,
#                                                fig.width=6,
#                                                fig.height=6,
#                                                out.width="504px",
#                                                out.height="504px"))
#                      } )
# 
# knitr::opts_template$set( img_template )

knitr::opts_chunk$set(echo = F,fig.width=7,fig.height = 3,dpi=300,fig.align="center")
#theme_set(theme_light())
theme_set(theme_light(base_size=11))

```

```{r }
boot_object = params$object
challenge_single=boot_object$data
ordering=  names(sort(t(boot_object$mat[,"rank",drop=F])["rank",]))

```
This document presents a systematic report on a benchmark study. Input data comprises raw metric values for all algorithms and test cases. Generated plots are:

* Visualization of assessment data: Dot- and boxplots, podium plots and ranking heatmaps
* Visualization of ranking robustness:  Blob plots, Violin plots, significance maps and line plots


Analysis based on `r nrow(challenge_single)/length(unique(challenge_single[[attr(challenge_single,"algorithm")]]))` test cases which included `r sum(is.na(challenge_single[[attr(challenge_single,"value")]]))` missing values.

Algorithms are ordered according to chosen ranking scheme.

<!-- This text is outcommented -->
<!-- R code chunks start with "```{r }" and end with "```" -->
<!-- Please do not change anything inside of code chunks, otherwise any latex code is allowed -->

<!-- inline code with `r 0` -->



Ranking list:

```{r}
knitr::kable(boot_object$mat[order(boot_object$mat$rank),])

```







# Visualization of raw assessment data

## Dot- and boxplot

*Dot- and boxplots* for visualizing raw assessment data separately for each algorithm. Boxplots representing descriptive statistics over all test cases (median, quartiles and outliers) are combined with horizontally jittered dots representing individual test cases.
\bigskip

```{r boxplots}
boxplot(challenge_single,ranking.fun=boot_object$FUN,size=.8)+xlab("Algorithm")+ylab("Metric value")

```



## Podium plot
*Podium plots* (see also Eugster et al, 2008) for visualizing raw assessment data. Upper part (spaghetti plot): Participating algorithms are color-coded, and each colored dot in the plot represents a metric value achieved with the respective algorithm. The actual metric value is encoded by the y-axis. Each podium (here: $p$=`r length(ordering)`) represents one possible rank, ordered from best (1) to last (here: `r length(ordering)`). The assignment of metric values (i.e. colored dots) to one of the podiums is based on the rank that the respective algorithm achieved on the corresponding test case. Note that the plot part above each podium place is further subdivided into $p$ "columns", where each column represents one participating algorithm (here: $p=$ `r length(ordering)`).  Dots corresponding to identical test cases are connected by a line, leading to the shown spaghetti structure. Lower part (podium plots): Bar charts represent the relative frequency for each algorithm to achieve the rank encoded by the podium place. 
\bigskip

```{r podium,eval=T,fig.width=12, fig.height=6}
# gg_color_hue <- function(n) {
#   hues = seq(15, 375, length = n + 1)
#   hcl(h = hues, l = 65, c = 100)[1:n]
# }
# 
# cols = gg_color_hue(n)

# Add extra space to right of plot area; change clipping to figure
# par(mar=c(5, 4, 4, 2) + 0.1)
# op <-  par(mar=par()$mar+c(0,0,0,5))
#   set.seed(38)
#  podium(challenge_single,
#         ranking.fun=boot_object$FUN, 
#         lines.show = T,lines.alpha = .3,
#         dots.cex=.8,ylab="metric value",
#           legendfn = function(algs, cols) {
#             #title(single_subtask); 
#             legend("topright", inset=c(-0.07,0), xpd=TRUE, algs, lwd = 1, col =  cols, bg = NA,cex=.6) 
#           }
#           )
#  par(op)

par(mar=c(5, 4, 4, 2) + 0.1,cex.axis=1.5,cex.lab=1.5,cex.main=1.7)
  set.seed(38)
op <-par(mar=par()$mar+c(-.5,0.5,-3.5,4.5))
 podium(challenge_single,
        ranking.fun=boot_object$FUN, 
        lines.show = T,lines.alpha = .4,
        dots.cex=.9,ylab="Metric value",layout.heights=c(1,.35),
          legendfn = function(algs, cols) {
            #title(single_subtask); 
            legend("topright", inset=c(-0.12,0), xpd=TRUE, algs, lwd = 1, col =  cols, bg = NA,cex=1.4,seg.len=1.1) 
          }
          )
par(op)
 
  
```


## Ranking heatmap
*Ranking heatmaps* for visualizing raw assessment data. Each cell $\left( i, A_j \right)$ shows the absolute frequency of test cases in which algorithm $A_j$ achieved rank $i$.

\bigskip

```{r rankingHeatmap,fig.width=9, fig.height=9,out.width='50%'}
#
rankingHeatmap(boot_object$data,ranking.fun=boot_object$FUN,fontsize=25, vp =grid::viewport(width=1, height=.95))
#
```



# Visualization of ranking stability



<!-- Results based on `r ncol(boot_object$bootsrappedRanks)` bootstrap samples. -->

## *Blob plot* for visualizing ranking stability based on bootstrap sampling

Algorithms are color-coded, and the area of each blob at position $\left( A_i, \text{rank } j \right)$ is proportional to the relative frequency $A_i$ achieved rank $j$ across $b=$ `r ncol(boot_object$bootsrappedRanks)` bootstrap samples. The median rank for each algorithm is indicated by a black cross. 95\% bootstrap intervals across bootstrap samples are indicated by black lines. 

\bigskip

```{r blobplot,fig.width=7,fig.height = 7}
stability2(boot_object,max_size = 8,ordering=ordering,size.ranks=.25*theme_get()$text$size,size=8,shape=4 )
```


## Violin plot for visualizing ranking stability based on bootstrapping

The ranking list based on the full assessment data is pairwisely compared with the ranking lists based on the individual bootstrap samples (here $b=$ `r ncol(boot_object$bootsrappedRanks)` samples). For each pair of rankings, Kendall's $\tau$ correlation is computed. Kendall’s $\tau$ is a scaled index determining the correlation between the lists. It is computed by evaluating the number of pairwise concordances and discordances between ranking lists and produces values between $-1$ (for inverted order) and $1$ (for identical order). A violin plot, which simultaneously depicts a boxplot and a density plot, is generated from the results.
\bigskip

```{r violin}
violin(boot_object)+xlab("")
```





## *Significance maps* for visualizing ranking stability based on statistical significance

*Significance maps* depict incidence matrices of
pairwise significant test results for the one-sided Wilcoxon signed rank test at a 5\% significance level with adjustment for multiple testing according to Holm. Blue shading indicates that metric values of the algorithm on the x-axis were significantly larger than those from the algorithm on the y-axis, red color indicates no significant difference.
\bigskip

```{r significancemap,fig.width=3.5, fig.height=3,out.width='70%'}
a=challenge_single%>%decision.challenge(alpha=0.05,p.adjust.method="holm")
relensemble= as.relation(a)

print(significanceMap(rankedMat=boot_object$mat,
                 relation_object=relensemble
                 )
        )#+ggtitle(Task))#+scale_color_manual(name="decision",values=c("white","lightblue")))

```




<!-- \subsubsection{Hasse diagram} -->

<!-- ```{r single_stability_significance_hasse, fig.height=19} -->
<!-- plot(relensemble) -->
<!-- ``` -->




## Ranking robustness with respect to ranking methods
*Line plots* for visualizing rankings robustness across different ranking methods. Each algorithm is represented by one colored line. For each ranking method encoded on the x-axis, the height of the line represents the corresponding rank. Horizontal lines indicate identical ranks for all methods.

\bigskip

```{r lineplot,fig.width=7,fig.height = 5}
methodsplot(challenge_single, ordering =ordering )
```





# Reference


Wiesenfarth, M., Reinke, A., Landmann A.L., Cardoso, M.J., Maier-Hein, L. and Kopp-Schneider, A. (2019). Methods and open-source toolkit for analyzing and visualizing challenge results. *arXiv preprint arXiv:1910.05121*


M. J. A. Eugster, T. Hothorn, and F. Leisch, “Exploratory
and inferential analysis of benchmark experiments,”
Institut fuer Statistik, Ludwig-Maximilians-
Universitaet Muenchen, Germany, Technical Report 30,
2008. [Online]. Available: http://epub.ub.uni-muenchen.
de/4134/.









